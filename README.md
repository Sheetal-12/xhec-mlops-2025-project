<div align="center">

# MLOps Project: Abalone Age Prediction

[![Python Version](https://img.shields.io/badge/python-3.10%20or%203.11-blue.svg)]()
[![Linting: ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/charliermarsh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)
[![Pre-commit](https://img.shields.io/badge/pre--commit-enabled-informational?logo=pre-commit&logoColor=white)](https://github.com/artefactory/xhec-mlops-project-student/blob/main/.pre-commit-config.yaml)
</div>

## üéØ Project Overview

Welcome to your MLOps project! In this hands-on project, you'll build a complete machine learning system to predict the age of abalone (a type of sea snail) using physical measurements instead of the traditional time-consuming method of counting shell rings under a microscope.

**Our Mission**: Transform a simple ML model into a production-ready system with automated training, deployment, and prediction capabilities.

## üìä About the Dataset

Traditionally, determining an abalone's age requires:
1. Cutting the shell through the cone
2. Staining it
3. Counting rings under a microscope (very time-consuming!)

**Our Goal**: Use easier-to-obtain physical measurements (shell weight, diameter, etc.) to predict the age automatically.

üì• **Dataset Download**: Get the dataset from the [Kaggle page](https://www.kaggle.com/datasets/rodolfomendes/abalone-dataset)

PR_0:

## üõ† Development Setup

To set up your environment:

```bash
uv sync
source .venv/bin/activate
pre-commit install
pre-commit run --all-files
```

This project uses:

- Ruff for linting and formatting

- Pre-commit for code quality checks

- Prefect for pipeline orchestration

- FastAPI for serving predictions

PR_1:

# Exploratory Data Analysis & Modeling

This part contains Jupyter notebooks for exploring the Abalone dataset and building predictive models to estimate the age of abalone based on physical measurements.

## Overview

The Abalone dataset contains physical measurements of abalone (a type of sea snail) used to predict their age. Age is determined by counting the number of rings on the shell, which corresponds to approximately one year of growth after the first 1.5 years of life.

**Target Variable**: `rings` (number of rings on the shell)
- Biological age ‚âà rings + 1.5 years

## Notebooks

### 1. `eda.ipynb` - Exploratory Data Analysis

Comprehensive exploration of the Abalone dataset including:

- **Data Quality Checks**
  - 4,177 samples with 9 features
  - No missing values
  - No duplicates

- **Feature Analysis**
  - **Categorical**: Sex (M/F/I - Male/Female/Infant)
  - **Numerical**: Length, Diameter, Height, Whole Weight, Shucked Weight, Viscera Weight, Shell Weight
  - **Target**: Rings (1-29, median=9, mean‚âà10)

- **Key Findings**
  - Right-skewed distributions for most features (especially weights)
  - Moderate correlation between size/weight and age (0.42-0.63)
  - High multicollinearity among size features (0.90-0.99)
  - Sex-based differences: Infants are smaller and younger
  - Outliers detected and removed using IQR method (164 rows removed)

- **Data Outputs**
  - Clean dataset saved to `../data/abalone_clean.csv`

### 2. `model.ipynb` - Linear Regression Modeling

Initial modeling experiments with MLflow tracking:

- **Model**: Linear Regression
- **Feature Engineering**
  - One-hot encoding for Sex variable
  - Comparison of scalers: StandardScaler, MinMaxScaler, RobustScaler, None

- **Experiments**
  - Multiple train/test splits (80/20, 70/30)
  - MLflow tracking for reproducibility
  - Experiment name: `Abalone_Age_Prediction_Multiple_Runs`

- **Performance**
  - MAE: ~1.54-1.55 rings
  - MSE: ~4.46-4.53
  - R¬≤: ~0.54-0.55
  - Scaling method had minimal impact on Linear Regression performance

## Getting Started

### Running the Notebooks

1. **EDA First**: Run `eda.ipynb` to generate the cleaned dataset
2. **Modeling**: Run `model.ipynb` to train models and log experiments

### Data Location

- Raw data: `../data/abalone.csv`
- Cleaned data: `../data/abalone_clean.csv` (generated by EDA notebook)

### MLflow Tracking

Model experiments are logged to `mlruns/` directory. To view the MLflow UI:
```bash
mlflow ui
```

Then navigate to `http://localhost:5000` in your browser.

## Key Insights

1. **Age Prediction Complexity**: The moderate R¬≤ (~0.55) suggests that physical measurements alone capture only part of the age variance. Environmental factors and genetics likely play important roles.

2. **Feature Importance**: Shell weight shows the strongest correlation with age (0.63), followed by diameter (0.58) and length (0.56).

3. **Data Quality**: The dataset is clean with balanced sex distribution, making it suitable for modeling without extensive preprocessing.

4. **Outlier Impact**: Removing outliers improved data quality by eliminating extreme measurements that could skew model training.

## Next Steps

Potential improvements for future iterations:

- Try non-linear models (Random Forest, XGBoost, Neural Networks)
- Feature engineering (ratios, polynomial features)
- Handle multicollinearity (PCA, feature selection)
- Cross-validation for more robust evaluation
- Ensemble methods
- Hyperparameter tuning

## File Structure
```
notebooks/
‚îú‚îÄ‚îÄ eda.ipynb                  # Exploratory Data Analysis
‚îú‚îÄ‚îÄ model.ipynb                # Linear Regression modeling
‚îú‚îÄ‚îÄ mlruns/                    # MLflow experiment tracking
‚îî‚îÄ‚îÄ README.md                  # This file
```

## References

- Dataset: UCI Machine Learning Repository - Abalone Dataset
- Features measured in standardized units (likely millimeters for length/diameter/height, grams for weights)

PR_2:

‚öôÔ∏è Data Preprocessing, Feature Engineering & Model Training

This stage focuses on preparing the Abalone dataset for machine learning, engineering meaningful features, and training baseline models with consistent preprocessing.

The implementation introduces structured preprocessing pipelines and improves experiment reproducibility and efficiency.

üßπ Data Preprocessing

Data preparation is implemented in Python scripts under src/modelling/.
The key preprocessing steps include:

Column Standardization

Convert column names to snake_case

Ensure consistent naming conventions across scripts

Target Variable Transformation

Age = rings + 1.5 (biological approximation)

Target remains as rings for modeling simplicity

Outlier Removal

Outliers in numeric columns are removed using the IQR (Interquartile Range) method

This step improves model robustness

Encoding

One-hot encoding applied to the categorical feature sex

Produces columns: sex_F, sex_I, sex_M

Feature Scaling

Numeric columns scaled with StandardScaler

Ensures that all features contribute equally to model training

üß© Feature Engineering

Derived features include:

Ratios such as shell_weight / whole_weight

Log transformations for skewed distributions

Engineered features improve correlations with rings and model performance

Redundant or highly correlated features (multicollinearity) are monitored and addressed during modeling

ü§ñ Model Training Pipeline

Training logic is implemented in:

src/modelling/main.py

The script handles:

Data loading and preprocessing

Train-test split (default 80/20)

Building a scikit-learn Pipeline

Fitting a Linear Regression model

Evaluating model performance using MAE, MSE, and R¬≤

All transformations are applied consistently across training and test data via the pipeline.

üìà Model Evaluation Results
Metric   Result (Linear Regression) Interpretation
MAE   ~1.55 rings Average prediction error
MSE   ~4.5  Variance of prediction errors
R¬≤ ~0.55 Model explains ~55% of target variance

‚úÖ The baseline model provides a solid foundation for later improvements using non-linear models.

üß™ Experiment Tracking with MLflow

Each model training run is logged with MLflow for reproducibility.

To launch MLflow UI and inspect results:

mlflow ui

Then open: http://localhost:5000

Tracked parameters include:

Model type

Data split ratio

Scaling method

Evaluation metrics (MAE, MSE, R¬≤)

‚ñ∂Ô∏è How to Run This Stage
Option 1: Run the training script manually
python src/modelling/main.py data/abalone.csv

Option 2: Run notebooks (EDA & Model)
jupyter notebook notebooks/eda.ipynb
jupyter notebook notebooks/model.ipynb

Option 3: View experiment results in MLflow
mlflow ui

Then open your browser at http://localhost:5000

üß† Insights

 - Linear models provide moderate predictive power ‚Äî physical measurements explain only part of age variability

 - Shell-related features are the most predictive

 - Scaling and one-hot encoding improve model stability

 - The pipeline ensures reproducibility and consistent preprocessing

üöÄ Next Steps

 - Integrate with Prefect for automated pipeline orchestration (see PR_3)

 - Add non-linear models like Random Forest, XGBoost, and CatBoost

 - Introduce feature selection and cross-validation

 - Automate retraining and monitoring workflows

PR_3:

## üîÑ Training Pipeline with Prefect

### Option 1: Run Training Flow Directly (One-Time)

**Using the new Prefect flow:**
```bash
python src/modelling/train_flow.py data/abalone_clean.csv
```

### Option 2: Automated Retraining with Prefect Deployment

#### Start Prefect Server

In a **separate terminal**, start the Prefect server (in "src" folder):
```bash
prefect server start
```

The Prefect UI will be available at: **http://127.0.0.1:4200**

#### Create and Run the Deployment

In another terminal, activate your virtual environment and run:
```bash
python src/modelling/deploy.py
```

This will:
- ‚úÖ Create a deployment named "abalone-weekly-retraining"
- ‚úÖ Schedule it to run **every Sunday at 2:00 AM**
- ‚úÖ Keep the deployment server running (leave this terminal open)

**Note:** Keep this terminal running to maintain the deployment. Press `Ctrl+C` to stop.

#### Manually Trigger a Deployment Run

You can trigger the deployment without waiting for the schedule:
```bash
# List all deployments
prefect deployment ls

# Trigger the deployment
prefect deployment run 'Train Abalone Age Prediction Model/abalone-weekly-retraining'
```

### Visualize Flows in Prefect UI

1. Open **http://127.0.0.1:4200** in your browser
2. Navigate to:
   - **Flows** ‚Üí See all registered flows
   - **Flow Runs** ‚Üí View execution history with detailed logs
   - **Deployments** ‚Üí Manage scheduled runs
   - **Work Pools & Workers** ‚Üí Monitor deployment infrastructure

3. Click on any flow run to see:
   - üìä Task execution timeline and DAG visualization
   - üìù Detailed logs for each task
   - ‚öôÔ∏è Input parameters and output results
   - üìà Performance metrics (MAE, MSE, R¬≤)
   - ‚è±Ô∏è Task duration and retry information

### Monitor Flow Runs
```bash
# View recent flow runs
prefect flow-run ls

# Get details of a specific run
prefect flow-run inspect <flow-run-id>

# View logs
prefect flow-run logs <flow-run-id>
```

## üèóÔ∏è Understanding the Training Pipeline Architecture

The Prefect training pipeline (`train_flow.py`) consists of these orchestrated tasks:

1. **Load and Preprocess Data** (with retry logic)
   - Reads CSV file
   - Converts columns to snake_case
   - Calculates age from rings
   - Removes IQR outliers
   - One-hot encodes the 'sex' column

2. **Prepare Features and Target**
   - Separates features (X) from target (y = rings)

3. **Split Train Test**
   - Creates 80/20 train-test split with reproducible random seed

4. **Build Pipeline**
   - Creates sklearn ColumnTransformer for numeric scaling
   - Passes through one-hot encoded sex columns
   - Wraps preprocessing + LinearRegression in Pipeline

5. **Train Model**
   - Fits the complete pipeline on training data

6. **Evaluate Model**
   - Calculates MAE, MSE, and R¬≤ metrics on test set

7. **Save Model**
   - Pickles the trained pipeline to `src/web_service/local_objects/model.pkl`

### Key Benefits of the Prefect Implementation

- ‚úÖ **Observability**: See exactly which tasks succeeded/failed and why
- ‚úÖ **Retry Logic**: Automatically retry data loading if it fails (network issues, etc.)
- ‚úÖ **Scheduling**: Automated weekly retraining without manual intervention
- ‚úÖ **Logging**: Centralized, structured logs for all tasks
- ‚úÖ **Parameterization**: Easy to change test_size, random_state, or paths
- ‚úÖ **Monitoring**: Track model performance metrics over time in the UI
- ‚úÖ **Reproducibility**: All parameters and results are logged for each run
